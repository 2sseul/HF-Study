# 2. USING TRANSFORMERS

- íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë“¤ì—” ì¼ë°˜ì ìœ¼ë¡œ ë§¤ìš° í¬ê¸° ë•Œë¬¸ì—, ëª¨ë“  íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ì„ ë¡œë“œí•˜ê³ , í•™ìŠµí•˜ê³ , ì €ì¥í•˜ëŠ” ë‹¨ì¼ APIë¥¼ ì œê³µí•˜ëŠ” íŠ¸ëœìŠ¤í¬ë¨¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ íƒ„ìƒí•˜ê²Œ ëë‹¤.

## 2-1. Behind the pipeline

![alt text](./assets/image.png)

### ğŸ” Pipelineì˜ 3ë‹¨ê³„ êµ¬ì¡°

```
ì „ì²˜ë¦¬ (Preprocessing)
    â†“
ëª¨ë¸ í†µê³¼ (Passing inputs through the model)
    â†“
í›„ì²˜ë¦¬ (Postprocessing)
```

### 1ë‹¨ê³„: í† í¬ë‚˜ì´ì €ë¥¼ ì´ìš©í•œ ì „ì²˜ë¦¬ (Preprocessing with a tokenizer)

### ì™œ ì „ì²˜ë¦¬ê°€ í•„ìš”í• ê¹Œ?

ë‹¤ë¥¸ ì‹ ê²½ë§ë“¤ê³¼ ë§ˆì°¬ê°€ì§€ë¡œ, **íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ì€ ì›ì‹œ í…ìŠ¤íŠ¸ë¥¼ ì§ì ‘ ì²˜ë¦¬í•  ìˆ˜ ì—†ë‹¤**. ë”°ë¼ì„œ íŒŒì´í”„ë¼ì¸ì˜ ì²« ë²ˆì§¸ ë‹¨ê³„ëŠ” í…ìŠ¤íŠ¸ ì…ë ¥ì„ ëª¨ë¸ì´ ì´í•´í•  ìˆ˜ ìˆëŠ” ìˆ«ìë¡œ ë³€í™˜í•˜ëŠ” ê²ƒ

### í† í¬ë‚˜ì´ì €ì˜ ì—­í• 

- **í† í°ìœ¼ë¡œ ë¶„í• **: ì…ë ¥ì„ ë‹¨ì–´, ë¶€ë¶„ë‹¨ì–´, ë˜ëŠ” ê¸°í˜¸(êµ¬ë‘ì  ê°™ì€)ë¡œ ë‚˜ëˆ„ê¸° (ì´ë¥¼ í† í°ì´ë¼ê³  í•¨)
- **ì •ìˆ˜ ë§¤í•‘**: ê° í† í°ì„ ì •ìˆ˜ë¡œ ë§¤í•‘
- **ì¶”ê°€ ì…ë ¥**: ëª¨ë¸ì— ìœ ìš©í•  ìˆ˜ ìˆëŠ” ì¶”ê°€ ì…ë ¥ë“¤ ì¶”ê°€

> ëª¨ë“  ì „ì²˜ë¦¬ëŠ” **ëª¨ë¸ì´ ì‚¬ì „ í›ˆë ¨ë  ë•Œì™€ ì •í™•íˆ ê°™ì€ ë°©ì‹**ìœ¼ë¡œ ìˆ˜í–‰ë˜ì–´ì•¼ í•œë‹¤.

### AutoTokenizer ì‚¬ìš©í•˜ê¸°

```python
from transformers import AutoTokenizer

# sentiment-analysis íŒŒì´í”„ë¼ì¸ì˜ ê¸°ë³¸ ì²´í¬í¬ì¸íŠ¸ ì‚¬ìš©
checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
```

### í† í¬ë‚˜ì´ì§• ì‹¤í–‰

```python
raw_inputs = [
    "I've been waiting for a HuggingFace course my whole life.",
    "I hate this so much!",
]

inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors="pt")
print(inputs)
```

### `return_tensors` ë§¤ê°œë³€ìˆ˜

ë°˜í™˜ë°›ì„ í…ì„œì˜ íƒ€ì…ì„ ì§€ì •

```python
inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors="pt")
print(inputs)
tokenizer.decode(inputs["input_ids"][0]) # ë””ì½”ë”©ë„ ê°€ëŠ¥
```

- `padding=True`: ì§§ì€ ì‹œí€€ìŠ¤ë¥¼ ê¸¸ê²Œ ë§ì¶¤
- `truncation=True`: ê¸´ ì‹œí€€ìŠ¤ë¥¼ ìë¦„ (BERTëŠ” ìµœëŒ€ 512 í† í°ê¹Œì§€ë§Œ ì²˜ë¦¬í•  ìˆ˜ ìˆì–´ì„œ, ë” ê¸´ ë¬¸ì¥ì€ ì˜ë¼ëƒ„)
- `return_tensors="pt"`: PyTorch í…ì„œë¡œ ë°˜í™˜ (TensorFlowëŠ” "tf", íƒ€ì…ì„ ì§€ì •í•˜ì§€ ì•Šìœ¼ë©´ ë¦¬ìŠ¤íŠ¸ì˜ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜)

### PyTorch í…ì„œ ê²°ê³¼

```python
{
    'input_ids': tensor([
        [ 101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102],
        [ 101, 1045, 5223, 2023, 2061, 2172, 999, 102, 0, 0, 0, 0, 0, 0, 0, 0]
    ]),
    'attention_mask': tensor([
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]
    ])
}
```

- **`input_ids`**: ê° ë¬¸ì¥ì˜ í† í°ë“¤ì˜ ê³ ìœ  ì‹ë³„ìë¥¼ í¬í•¨í•˜ëŠ” ë‘ í–‰ì˜ ì •ìˆ˜
- **`attention_mask`**: ì´ ì¥ì˜ ë’·ë¶€ë¶„ì—ì„œ ì„¤ëª…

### `attention_mask`ê°€ í•„ìš”í•œ ì´ìœ 

```python
texts = [
    "I love this!", # ì§§ì€ ë¬¸ì¥ (4ê°œ í† í°)
    "I've been waiting for a HuggingFace course my whole life."  # ê¸´ ë¬¸ì¥ (16ê°œ í† í°)
]
```

- ì»´í“¨í„°ëŠ” íš¨ìœ¨ì„±ì„ ìœ„í•´ í•œ ë²ˆì— ì—¬ëŸ¬ ë¬¸ì¥ì„ ì²˜ë¦¬í•˜ë ¤ê³  í•¨
- í•˜ì§€ë§Œ ëª¨ë“  ë¬¸ì¥ì˜ ê¸¸ì´ê°€ ë‹¬ë¼ì„œ ë°°ì¹˜ë¡œ ì²˜ë¦¬í•  ìˆ˜ ì—†ìŒ
- íŒ¨ë”©(Padding) - ì§§ì€ ë¬¸ì¥ì„ ê¸´ ë¬¸ì¥ ê¸¸ì´ì— ë§ì¶° 0ìœ¼ë¡œ ì±„ì›Œì„œ í•´ê²°í•˜ì

```python
# íŒ¨ë”© ì „
sentence1: [101, 1045, 2293, 2023, 999, 102]              # ê¸¸ì´ 6
sentence2: [101, 1045, 1005, 2310, ..., 2166, 1012, 102]  # ê¸¸ì´ 16

# íŒ¨ë”© í›„
sentence1: [101, 1045, 2293, 2023, 999, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  # ê¸¸ì´ 16
sentence2: [101, 1045, 1005, 2310, ..., 2166, 1012, 102, 0, 0, 0, 0, 0, 0]  # ê¸¸ì´ 16

```

- ëª¨ë¸ì´ íŒ¨ë”© í† í°(0)ì„ ì‹¤ì œ ë‹¨ì–´ë¡œ ì°©ê°í•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì—,

- `attention_mask`ë¥¼ í†µí•´ ì‹¤ì œ ë‹¨ì–´(í™œìš©í•  í† í°)ëŠ” 1ë¡œ, íŒ¨ë”© í† í°ì€ 0ìœ¼ë¡œ ì„¤ì •í•´ íŒ¨ë”© í† í°ì€ ë¬´ì‹œí•œë‹¤.

---

### 2ë‹¨ê³„: ëª¨ë¸ í†µê³¼ (Going through the model)

### ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ

í† í¬ë‚˜ì´ì €ì™€ ê°™ì€ ë°©ì‹ìœ¼ë¡œ ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí•  ìˆ˜ ìˆë‹¤. TransformersëŠ” `from_pretrained()` ë©”ì„œë“œë¥¼ ê°€ì§„ `AutoModel` í´ë˜ìŠ¤ë¥¼ ì œê³µí•œë‹¤.

```python
from transformers import AutoModel

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
model = AutoModel.from_pretrained(checkpoint)

# í† í°í™”ëœ ìˆ«ìë“¤ì„ ë„£ìœ¼ë©´
outputs = base_model(**inputs)
print(outputs.last_hidden_state.shape)
# torch.Size([2, 16, 768])
```

### AutoModel & AutoModelForSequenceClassification

AutoModelë§Œìœ¼ë¡œëŠ” "ê¸ì •/ë¶€ì •"ì„ íŒë‹¨í•  ìˆ˜ ì—†ê¸° ë•Œë¬¸ì— AutoModelForSequenceClassification ì‚¬ìš©

```python
# AutoModel (ì˜ë¯¸ë§Œ íŒŒì•…, ê²°ë¡  ì—†ìŒ)
base_model = AutoModel.from_pretrained(checkpoint)
outputs = base_model(**inputs)
print(outputs.shape)  # [2, 8, 768] - ê° í† í°ë§ˆë‹¤ 768ê°œ ì˜ë¯¸ë²¡í„°

# AutoModelForSequenceClassification (ì˜ë¯¸ íŒŒì•… + ê²°ë¡ )
classifier_model = AutoModelForSequenceClassification.from_pretrained(checkpoint)
outputs = classifier_model(**inputs)
print(outputs.logits.shape)  # [2, 2] - ë¬¸ì¥ë³„ë¡œ [ë¶€ì •ì ìˆ˜, ê¸ì •ì ìˆ˜]
```

```
í† í°ë“¤ â†’ ê¸°ë³¸ ë¸”ë¡ â†’ 768ì°¨ì› ë²¡í„°ë“¤ â†’ ë¶„ë¥˜ ë¸”ë¡ â†’ 2ê°œ ì ìˆ˜
[101,1045...] â†’ [0.1,-0.3...] â†’ ì„ í˜•ë³€í™˜ â†’ [-1.56, 1.61]
```

- **`AutoModel`**: ë‹¨ì–´ ì˜ë¯¸ë§Œ íŒŒì•… (768ì°¨ì› ë²¡í„°ë“¤)
- **`AutoModelForSequenceClassification`**: ì˜ë¯¸ íŒŒì•… + ê°ì • íŒë‹¨ (2ê°œ ì ìˆ˜)
- Pipelineì€ ë‘ ë²ˆì§¸ ê±¸ ì‚¬ìš©í•´ì„œ ë°”ë¡œ ë‹µì„ ì¤€ë‹¤

### ëª¨ë¸ ì…ë ¥ ì°¨ì› ë¬¸ì œ

```python
# ì˜ëª»ëœ ë°©ë²• - 1ì°¨ì› í…ì„œ
input_ids = torch.tensor(ids)
model(input_ids)  # ì—ëŸ¬ ë°œìƒ!

# ì˜¬ë°”ë¥¸ ë°©ë²• - 2ì°¨ì› í…ì„œ (ë°°ì¹˜ í˜•íƒœ)
input_ids = torch.tensor([ids])  # ë°°ì¹˜ ì°¨ì› ì¶”ê°€
model(input_ids)  # ì •ìƒ ì‘ë™
```

ëª¨ë¸ì€ ê¸°ë³¸ì ìœ¼ë¡œ ì—¬ëŸ¬ ë¬¸ì¥(ë°°ì¹˜)ì„ ì…ë ¥ë°›ë„ë¡ ì„¤ê³„ë˜ì–´ ìˆì–´ì„œ, ë‹¨ì¼ ë¬¸ì¥ì´ë¼ë„ ë°°ì¹˜ ì°¨ì›ì„ ì¶”ê°€í•´ì•¼ í•¨

---

### 3ë‹¨ê³„: ì¶œë ¥ í›„ì²˜ë¦¬

ëª¨ë¸ì˜ ì¶œë ¥ìœ¼ë¡œ ì–»ëŠ” ê°’ë“¤ì€ ê·¸ ìì²´ë¡œëŠ” ì˜ë¯¸ê°€ ìˆì§€ ì•Šë‹¤.

```python
print(outputs.logits)
# tensor([[-1.5607,  1.6123],
#         [ 4.1692, -3.3464]], grad_fn=<AddmmBackward>)
```

**ì´ê²ƒë“¤ì€ í™•ë¥ ì´ ì•„ë‹ˆë¼ ë¡œì§“ì´ë‹¤.** ì¦‰ ëª¨ë¸ì˜ ë§ˆì§€ë§‰ ë ˆì´ì–´ì—ì„œ ì¶œë ¥ëœ ì›ì‹œ, ì •ê·œí™”ë˜ì§€ ì•Šì€ ì ìˆ˜ë‹¤.

### í™•ë¥ ë¡œ ë³€í™˜: SoftMax

ë¡œì§“ì„ í™•ë¥ ë¡œ ë³€í™˜í•˜ë ¤ë©´ **[SoftMax](https://en.wikipedia.org/wiki/Softmax_function) ë ˆì´ì–´**ë¥¼ ê±°ì³ì•¼ í•œë‹¤.

> Softmax: ì§€ìˆ˜í•¨ìˆ˜ë¡œ ëª¨ë“  ê°’ì„ ì–‘ìˆ˜ë¡œ ë§Œë“¤ê³ , ì „ì²´ í•©ìœ¼ë¡œ ë‚˜ëˆ ì„œ í•©ì„ 1ë¡œ ë§Œë“¦

```python
import torch

predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)
print(predictions)
# tensor([[4.0195e-02, 9.5980e-01],
#         [9.9946e-01, 5.4418e-04]], grad_fn=<SoftmaxBackward>)
```

### ë ˆì´ë¸” ë§¤í•‘

- ì²« ë²ˆì§¸ ë¬¸ì¥: `[0.0402, 0.9598]`
- ë‘ ë²ˆì§¸ ë¬¸ì¥: `[0.9995, 0.0005]`

ê° ìœ„ì¹˜ì— í•´ë‹¹í•˜ëŠ” ë ˆì´ë¸”ì„ ì–»ìœ¼ë ¤ë©´ ëª¨ë¸ ì„¤ì •ì˜ `id2label` ì†ì„±ì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤:

```python
model.config.id2label
# {0: 'NEGATIVE', 1: 'POSITIVE'}
```

## 2-2. Models

### 1. ëª¨ë¸ ë¡œë“œí•˜ê¸°

```python
from transformers import AutoModel # ìë™ ë¡œë“œ
model = AutoModel.from_pretrained("bert-base-cased")

from transformers import BertModel # ëª¨ë¸ íƒ€ì… ë¡œë“œ
model = BertModel.from_pretrained("bert-base-cased")
```

- `AutoModel`ì€ ì²´í¬í¬ì¸íŠ¸ ì´ë¦„ì„ ë³´ê³  ìë™ìœ¼ë¡œ ì ì ˆí•œ ëª¨ë¸ì„ ì„ íƒí•´ì£¼ëŠ” í¸ë¦¬í•œ í´ë˜ìŠ¤
- "bert-base-cased"ëŠ” BERT ëª¨ë¸ì˜ ê¸°ë³¸ êµ¬ì¡°(12ì¸µ, 768 ì€ë‹‰í¬ê¸°, 12ê°œ ì–´í…ì…˜ í—¤ë“œ)ë¥¼ ì˜ë¯¸
- íŠ¹ì • ëª¨ë¸ì„ ì§ì ‘ ì‚¬ìš©í•˜ê³  ì‹¶ìœ¼ë©´ `ëª¨ë¸ íƒ€ì…ìœ¼ë¡œ ë¡œë“œ(Bert.from_preterained)`

### 2. ëª¨ë¸ ì €ì¥í•˜ê³  ë¶ˆëŸ¬ì˜¤ê¸°

**ëª¨ë¸ ì €ì¥í•˜ê³  ë¶ˆëŸ¬ì˜¤ê¸°**

```python
# ëª¨ë¸ ì €ì¥í•˜ê¸°
model.save_pretrained("src")

# ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
model = AutoModel.from_pretrained("src")
```

**Hugging Face Hubì— ê³µìœ í•˜ê¸°**

```python
# ë¡œê·¸ì¸ í›„
from huggingface_hub import notebook_login notebook_login()
model.push_to_hub("ë‚´-ë©‹ì§„-ëª¨ë¸~")

# ë‹¤ë¥¸ ì‚¬ëŒì´ ì‚¬ìš©í•  ë•Œ
model = AutoModel.from_pretrained("ì‚¬ìš©ìëª…/ë‚´-ë©‹ì§„-ëª¨ë¸~")
```

---

## 2-3. Tokenizers

### í† í¬ë‚˜ì´ì œì´ì…˜ ì•Œê³ ë¦¬ì¦˜ ì¢…ë¥˜

**1. `ë‹¨ì–´ ê¸°ë°˜ í† í¬ë‚˜ì´ì œì´ì…˜ (Word-based)`**

```python
tokenized_text = "Jim Henson was a puppeteer".split()
print(tokenized_text)
# ['Jim', 'Henson', 'was', 'a', 'puppeteer']
```

**ì¥ì :**

- ì„¤ì •ì´ ê°„ë‹¨í•˜ê³  ì‚¬ìš©í•˜ê¸° ì‰¬ì›€
- ì§ê´€ì ì„

**ë‹¨ì :**

- ì–´íœ˜ëŸ‰ì´ ë§¤ìš° í¼ (ì˜ì–´ë§Œ 50ë§Œ ë‹¨ì–´ ì´ìƒ)
- "dog"ì™€ "dogs"ë¥¼ ì™„ì „íˆ ë‹¤ë¥¸ ë‹¨ì–´ë¡œ ì¸ì‹
- ì–´íœ˜ì— ì—†ëŠ” ë‹¨ì–´ëŠ” [UNK] í† í°ìœ¼ë¡œ ì²˜ë¦¬

**2. `ë¬¸ì ê¸°ë°˜ í† í¬ë‚˜ì´ì œì´ì…˜ (Character-based)`**

```python
# "Hello" â†’ ['H', 'e', 'l', 'l', 'o']
```

**ì¥ì :**

- ì–´íœ˜ëŸ‰ì´ ì‘ìŒ
- [UNK] í† í°ì´ ê±°ì˜ ì—†ìŒ

**ë‹¨ì :**

- ê°œë³„ ë¬¸ìëŠ” ì˜ë¯¸ê°€ ì œí•œì 
- ì‹œí€€ìŠ¤ê°€ ë§¤ìš° ê¸¸ì–´ì§ (ë‹¨ì–´ í•˜ë‚˜ê°€ 10ê°œ ì´ìƒì˜ í† í°ìœ¼ë¡œ)

**3. `ì„œë¸Œì›Œë“œ í† í¬ë‚˜ì´ì œì´ì…˜ (Subword)` â­ ìµœì !**

ê°€ì¥ ì¢‹ì€ ë°©ë²•! ìì£¼ ì‚¬ìš©ë˜ëŠ” ë‹¨ì–´ëŠ” ë¶„í• í•˜ì§€ ì•Šê³ , ë“œë¬¸ ë‹¨ì–´ëŠ” ì˜ë¯¸ ìˆëŠ” ì„œë¸Œì›Œë“œë¡œ ë¶„í•´

```python
# "annoyingly" â†’ ["annoying", "ly"]
# "tokenization" â†’ ["token", "ization"]
```

**ì£¼ìš” ì•Œê³ ë¦¬ì¦˜:**

- `Byte-level BPE`: GPT-2ì—ì„œ ì‚¬ìš©
- `WordPiece`: BERTì—ì„œ ì‚¬ìš©
- `SentencePiece/Unigram`: ë‹¤êµ­ì–´ ëª¨ë¸ì—ì„œ ì‚¬ìš©

### í† í¬ë‚˜ì´ì € ì €ì¥ê³¼ ë¡œë”©

```python
from transformers import AutoTokenizer

# í† í¬ë‚˜ì´ì € ë¡œë”©
tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")

# í† í¬ë‚˜ì´ì € ì €ì¥
tokenizer.save_pretrained("my_tokenizer")

# ì €ì¥ëœ í† í¬ë‚˜ì´ì € ë¡œë”©
tokenizer = AutoTokenizer.from_pretrained("my_tokenizer")
```

### ì¸ì½”ë”© ê³¼ì • ìƒì„¸ ë¶„ì„

**1ë‹¨ê³„: í† í¬ë‚˜ì´ì œì´ì…˜**

```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
sequence = "Using a Transformer network is simple"

tokens = tokenizer.tokenize(sequence)
print(tokens)
# ['Using', 'a', 'transform', '##er', 'network', 'is', 'simple']
```

**2ë‹¨ê³„: í† í°ì„ IDë¡œ ë³€í™˜**

```python
ids = tokenizer.convert_tokens_to_ids(tokens)
print(ids)
# [7993, 170, 11303, 1200, 2443, 1110, 3014]
```

**ë””ì½”ë”©: IDë¥¼ ë‹¤ì‹œ í…ìŠ¤íŠ¸ë¡œ**

```python
decoded_string = tokenizer.decode([7993, 170, 11303, 1200, 2443, 1110, 3014])
print(decoded_string)
# 'Using a Transformer network is simple'
```

### íŠ¹ìˆ˜ í† í° ìë™ ì¶”ê°€

```python
sequence = "I've been waiting for a HuggingFace course my whole life."

# ì§ì ‘ í† í¬ë‚˜ì´ì§•
tokens = tokenizer.tokenize(sequence)
ids = tokenizer.convert_tokens_to_ids(tokens)
print(ids)
# [1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012]

# í† í¬ë‚˜ì´ì €ë¡œ í•œë²ˆì— ì²˜ë¦¬
model_inputs = tokenizer(sequence)
print(model_inputs["input_ids"])
# [101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102]

# ë””ì½”ë”©í•´ë³´ë©´
print(tokenizer.decode(model_inputs["input_ids"]))
# "[CLS] i've been waiting for a huggingface course my whole life. [SEP]"
```

**ìë™ìœ¼ë¡œ ì¶”ê°€ë˜ëŠ” íŠ¹ìˆ˜ í† í°:**

- `[CLS]`: ë¬¸ì¥ì˜ ì‹œì‘ (ID: 101)
- `[SEP]`: ë¬¸ì¥ì˜ ë (ID: 102)

---

## 2-4. ì—¬ëŸ¬ ì‹œí€€ìŠ¤ ì²˜ë¦¬í•˜ê¸°

### ë°°ì¹˜ ì²˜ë¦¬ì˜ í•„ìš”ì„±

```python
# ë‹¨ì¼ ì‹œí€€ìŠ¤ëŠ” ì—ëŸ¬!
input_ids = torch.tensor(ids)
model(input_ids)  # IndexError ë°œìƒ!

# ë°°ì¹˜ í˜•íƒœë¡œ ë§Œë“¤ì–´ì•¼ í•¨ - ë‹¨ì¼ì´ì–´ë„ 2ì°¨ì›ìœ¼ë¡œ ìƒì„±í•  ê²ƒ
input_ids = torch.tensor([ids])  # 2ì°¨ì›ìœ¼ë¡œ ë³€ê²½
model(input_ids)  # ì •ìƒ ì‘ë™
```

### íŒ¨ë”©ì´ í•„ìš”í•œ ì´ìœ 

```python
# ê¸¸ì´ê°€ ë‹¤ë¥¸ ë¬¸ì¥ë“¤
batched_ids = [
    [200, 200, 200],      # ê¸¸ì´ 3
    [200, 200]            # ê¸¸ì´ 2
]
# ì´ê±¸ í…ì„œë¡œ ë§Œë“¤ ìˆ˜ ì—†ìŒ (ì§ì‚¬ê°í˜•ì´ ì•„ë‹ˆë¯€ë¡œ)

# íŒ¨ë”©ìœ¼ë¡œ í•´ê²°
padding_id = 100
batched_ids = [
    [200, 200, 200],
    [200, 200, padding_id],  # íŒ¨ë”© ì¶”ê°€
]
```

### Attention Maskì˜ ì¤‘ìš”ì„±

íŒ¨ë”© í† í°ì„ ë¬´ì‹œí•˜ë„ë¡ ì–´í…ì…˜ ë§ˆìŠ¤í¬ ì‚¬ìš©

```python
batched_ids = [
    [200, 200, 200],
    [200, 200, tokenizer.pad_token_id],
]

attention_mask = [
    [1, 1, 1],    # ëª¨ë“  í† í° ì£¼ì˜
    [1, 1, 0],    # ë§ˆì§€ë§‰ í† í°(íŒ¨ë”©) ë¬´ì‹œ
]

outputs = model(
    torch.tensor(batched_ids),
    attention_mask=torch.tensor(attention_mask)
)
```

### ì‹œí€€ìŠ¤ ê¸¸ì´ ì œí•œ

ëŒ€ë¶€ë¶„ì˜ Transformer ëª¨ë¸ì€ **512-1024 í† í°** ì œí•œì´ ìˆìŒ:

```python
# ê¸´ ì‹œí€€ìŠ¤ ì²˜ë¦¬ ë°©ë²•
sequences = ["ë§¤ìš° ê¸´ í…ìŠ¤íŠ¸..."]

# ë°©ë²• 1: ì˜ë¼ë‚´ê¸°
model_inputs = tokenizer(sequences, truncation=True)

# ë°©ë²• 2: ê¸´ ì‹œí€€ìŠ¤ ì „ìš© ëª¨ë¸ ì‚¬ìš©
# - Longformer: 4096 í† í°ê¹Œì§€
# - LED: 16384 í† í°ê¹Œì§€
```

---

## 2-5. Putting it all together

í† í¬ë‚˜ì´ì €ì˜ ê°•ë ¥í•œ í†µí•© API: ìˆ˜ë™ìœ¼ë¡œ í•˜ë˜ ëª¨ë“  ì‘ì—…ì„ í† í¬ë‚˜ì´ì €ê°€ í•œë²ˆì— ì²˜ë¦¬

```python
from transformers import AutoTokenizer

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)

# ë‹¨ì¼ ì‹œí€€ìŠ¤
sequence = "I've been waiting for a HuggingFace course my whole life."
model_inputs = tokenizer(sequence)

# ì—¬ëŸ¬ ì‹œí€€ìŠ¤
sequences = [
    "I've been waiting for a HuggingFace course my whole life.",
    "So have I!"
]
model_inputs = tokenizer(sequences)
```

### ë‹¤ì–‘í•œ íŒ¨ë”© ì˜µì…˜

```python
# ê°€ì¥ ê¸´ ì‹œí€€ìŠ¤ì— ë§ì¶° íŒ¨ë”©
model_inputs = tokenizer(sequences, padding="longest")

# ëª¨ë¸ ìµœëŒ€ ê¸¸ì´ì— ë§ì¶° íŒ¨ë”© (512í† í°)
model_inputs = tokenizer(sequences, padding="max_length")

# ì§€ì •ëœ ê¸¸ì´ì— ë§ì¶° íŒ¨ë”©
model_inputs = tokenizer(sequences, padding="max_length", max_length=8)
```

### ì˜ë¼ë‚´ê¸° ì˜µì…˜

```python
# ëª¨ë¸ ìµœëŒ€ ê¸¸ì´ë¡œ ì˜ë¼ë‚´ê¸°
model_inputs = tokenizer(sequences, truncation=True)

# ì§€ì •ëœ ê¸¸ì´ë¡œ ì˜ë¼ë‚´ê¸°
model_inputs = tokenizer(sequences, max_length=8, truncation=True)
```

### í”„ë ˆì„ì›Œí¬ë³„ í…ì„œ ë°˜í™˜

```python
sequences = ["I've been waiting for a HuggingFace course my whole life.", "So have I!"]

# PyTorch í…ì„œ
model_inputs = tokenizer(sequences, padding=True, return_tensors="pt")

# NumPy ë°°ì—´
model_inputs = tokenizer(sequences, padding=True, return_tensors="np")

# TensorFlow í…ì„œ
model_inputs = tokenizer(sequences, padding=True, return_tensors="tf")
```

### ì™„ì „í•œ íŒŒì´í”„ë¼ì¸

```python
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)

sequences = [
    "I've been waiting for a HuggingFace course my whole life.",
    "So have I!"
]

# ëª¨ë“  ì „ì²˜ë¦¬ë¥¼ í•œë²ˆì—
tokens = tokenizer(
    sequences,
    padding=True,
    truncation=True,
    return_tensors="pt"
)

# ëª¨ë¸ì— ë°”ë¡œ ì…ë ¥
output = model(**tokens)
print(output.logits)
```
