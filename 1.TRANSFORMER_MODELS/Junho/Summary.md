## Natural Language Processing and Large Language Models

#### NLP
- 정의
    - 인간 언어와 관련된 모든 것을 이해하는 데 중점을 둔 언어학 및 머신러닝 분야
    - 단어 하나하나를 개별적으로 이해하는 것이 아닌, 그 단어들의 문맥을 이해하는 것이 목표
- 작업
    - 문장 전체 분류
        - e.g., 감성 분석, 스팸 이메일 분류, 문법 교정, 문장 간 관계성 파악
    - 문장 내 각 단어 분류
        - e.g., 문장의 문법적 구성 요소 또는 개체명 식별
    - 텍스트 콘텐츠 생성
        - e.g., 생성된 텍스트로 프롬프트 작성, 마스킹된 빈칸 채우기
    - 텍스트에서 답 추출
        - e.g., 질문에 대한 답 추출
    - 입력 텍스트에서 새로운 문장 생성
        - e.g., 다른 언어로 번역, 요약
- 발전
    - 텍스트 데이터에만 의존하는 것이 아닌 음성 데이터의 스크립트를 생성하거나 이미지에 대한 설명을 만드는 것과 같은 음성 인식 및 컴퓨터 비전 분야에서도 활용할 수 있음   
#### LLM
- 정의
    - 방대한 양의 텍스트 데이터로 학습되어 사람과 유사한 텍스트를 이해하고 생성하며, 언어에서 패턴을 인식하고, 작업별 훈련 없이도 다양한 언어 작업을 수행할 수 있는 AI 모델
- 특징
    - 규모: 수십억부터 수천억 개의 매개변수를 포함
    - 일반적인 기능: 특정 작업에 대한 훈련 없이도 여러 작업을 수행할 수 있음
    - 문맥 내 학습: 프롬프트에서 제공된 예시로부터 학습
    - 새로운 능력: 모델의 크기가 커짐에 따라 예상치 않았던 새로운 능력을 보일 수 있음
- 한계
    - 할루시네이션: 틀린 정보를 자신 있게 생성
    - 이해 부족: 이론적인 이해를 통한 도출이 아닌 단순히 통계적 패턴에 따라 작동
    - 편향: 학습데이터나 입력에 존재하는 편향에 의존
    - 문맥 한계: 문맥 창이 제한됨
    - 컴퓨팅 자원: 상당한 양의 컴퓨팅 자원이 필요

## Transformers, what can they do?

#### pipeline()
- 정의
    - 텍스트를 입력하면 바로 결과를 얻을 수 있도록 모델과 전처리 및 후처리 과정을 연결해주는 기능
- 동작 과정
    - 전처리: 입력 텍스트를 모델이 이해할 수 있는 형식으로 변환
        - e.g., 토큰화, 임베딩
    - 모델 처리: 전처리된 데이터가 모델에 전달되어 예측을 수행
        - e.g., 감성 분석으로 긍정인지 부정인지 계산
    - 후처리: 모델의 예측 결과를 사람이 이해할 수 있는 형태로 변환
        - e.g., POSITVE 라벨 + 확률 점수
- 작업
    - 텍스트
        - text-generation : 주어진 프롬프트에 따라 텍스트 생성
            - Decoder만 사용
            - e.g., GPT, LLaMA, etc.
        - text-classification : 텍스트를 미리 정해진 카테고리로 분류
            - Encoder만 사용
            - e.g., BERT, RoBERTa
        - summarization : 텍스트의 핵심 정보를 유지하면서 더 짧게 요약
            - Encoder-Decoder 사용
            - e.g., T5, BART
        - translation : 한 언어를 다른 언어로 번역
            - Encoder-Decoder 사용
            - e.g., T5, BART
        - zero-shot-classification : 특정 라벨에 대해 학습하지 않고도 텍스틀 분류
        - feature-extraction : 텍스트를 벡터형(수치형)으로 추출
        - NER
            - Encoder만 사용
            - e.g., BERT, RoBERTa
        - QA(Extractive)
            - Encoder만 사용
            - e.g., BERT, RoBERTa
        - QA(Generative)
            - Encoder-Decoder 또는 Decoder만 사용
            - e.g., T5, GPT
        - Conversational 
            - Decoder만 사용
            - e.g., GPT, LLaMA
    - 이미지
        - image-to-text : 이미지에 대한 설명을 텍스트로 생성
        - image-classification : 이미지 내 대상을 식별하고 분류
        - object-detection : 이미지 내 특정 객체의 위치와 종류를 찾아냄
    - 오디오
        - automatic-speech-recognition : 음성을 텍스트로 변환
        - audio-classification : 오디오를 특정 카테고리로 분류
        - text-to-speech : 텍스트를 음성으로 변환
    - 멀티모달
        - image-text-to-text : 텍스트 프롬프트와 이미지를 함께 입력받아 텍스트로 응답

## How Transformers slove tasks

#### Language Model의 작동 원리
- 작동 원리
    - 언어 모델은 주변 단어의 문맥을 기반으로 특정 단어가 나타날 확률을 예측하도록 훈련됨
    - Transformer 모델의 주요 접근법
        - 마스크 언어 모델링(MLM)
            - BERT와 같은 인코더 모델이 사용하는 방식
            - 입력 문장에서 일부 토큰을 무작위로 가리고, 주변 문맥을 이용해 원래 토큰을 예측하도록 훈련
        - 인과적 언어 모델링(CLM)
            - GPT와 같은 디코더 모델이 사용하는 방식
            - 시퀀스의 모든 이전 토큰을 기반으로 다음 토큰을 예측
            - 모델은 다음 토큰을 예측하기 위해 이전 토큰들로 구성된 문맥만 사용

#### 언어 모델의 종류
- 종류
    - 인코더 전용 모델
        - 양방향 접근법을 사용하여 양쪽 방향의 문맥을 이해함
        - 분류, 개체명 인식, 질문-답변과 같이 텍스트에 대한 깊은 이해가 필요한 작업에 적합
        - e.g., BERT
    - 디코터 전용 모델
        - 텍스트를 왼쪽에서 오른쪽으로 처리하며, 특히 텍스트 생성 작업에 특화됨
        - 프롬프트를 기반으로 문장을 완성하거나, 글을 쓰거나, 코드를 생성할 수 있음
        - e.g., GPT, Llama
    - 인코더-디코더 모델
        - 인코더로 입력을 이해하고 디코더로 출력을 생성하는 두 가지 접근법을 결합
        - 번역, 요약, 질문-답변과 같은 시퀀스-투-시퀀스 작업에 특화

